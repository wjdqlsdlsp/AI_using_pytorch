{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "과제4_Cliff Walking Example",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wjdqlsdlsp/AI_using_pytorch/blob/main/%EA%B3%BC%EC%A0%9C4_Cliff_Walking_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8Jl3uq0gg-E"
      },
      "source": [
        "# **[인공지능] 과제4 Cliff Walking 예제 구현**\n",
        "*   **QLearning Class 및 Sarsa Class를 완성하여 결과를 살펴보는 것이 목표**입니다.\n",
        "*   기본적인 코드는 아래 노트에 모두 작성되어 있습니다. 비어있는 함수 부분을 완성하면 됩니다.\n",
        "*   **과제 수행 시 주의사항: 외부 라이브러리로 Q-learning 및 Sarsa 적용하지 말 것, 수업 때 배운 내용대로 Q-learning과 Sarsa를 주어진 함수에 구현할 것.** 웹 상에 있는 다양한 Q-learning 및 Sarsa 코드를 참고하는 것은 괜찮습니다.\n",
        "*   **보고서 작성 내용**: 여러분이 완성한 Q-learning 및 sarsa 알고리즘의 내용과 결과의 의미를 분석하는 내용을 작성하면 됩니다.\n",
        "작성한 코드와 실행 결과를 첨부하길 바라며, 코드에는 자세한 주석을 필수적으로 포함하기 바랍니다. 보고서는 PDF로 제출바랍니다.\n",
        "*   보고서는 11월 30일 오후 11시 59분까지 블랙보드에 보고서 형태로 제출하면 됩니다. 지각은 0점입니다.\n",
        "\n",
        "# **본 노트를 본인의 drive로 복사하여 활용하기 바랍니다.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuK87dbPv7hW"
      },
      "source": [
        "본 과제는 OpenAI Gym 환경에 기반하여 작성되었습니다. Gym 라이브러리는 학습을 적용할 수 있는 다양한 환경을 제공합니다. 여기서는 수업에서 다뤘던 Cliff Walking 환경을 활용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlwSKTZhgDTM"
      },
      "source": [
        "import gym\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import random\n",
        "import itertools\n",
        "import sys\n",
        "from collections import defaultdict\n",
        "from gym.envs.toy_text.cliffwalking import CliffWalkingEnv # Cliff Walking 환경"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4XRfMCWq9GL"
      },
      "source": [
        "QtoPolicy Class는 학습된 Q-value를 입력하면 해당하는 Q-value의 greedy 정책이 출력되도록 하는 함수 `printPolicy`를 구성하는 Class입니다. Q-learning 및 Sarsa를 이용하여 학습된 정책을 출력하기 위해 필요합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLfl84A7nfo7"
      },
      "source": [
        "class QtoPolicy:\n",
        "    def __init__(self):\n",
        "        self.action = ['↑','→','↓','←']\n",
        "    \n",
        "    def printPolicy(self, Q):\n",
        "        policy = np.array([np.argmax(Q[key]) if key in Q else -1 for key in np.arange(48)])\n",
        "        v = ([np.max(Q[key]) if key in Q else 0 for key in np.arange(48)])\n",
        "        actions = np.stack([self.action for _ in range(len(policy))], axis=0)\n",
        "        \n",
        "        print(np.take(actions, np.reshape(policy, (4, 12))))\n",
        "        print('')"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "761Rn8P2rPR7"
      },
      "source": [
        "아래는 Q-learning 알고리즘을 수행하는 Class의 정의입니다.\n",
        "*   `update()` 메쏘드의 경우 state, action, reward, next_state, next_action이 주어졌을 때 Q-value를 업데이트하는 함수입니다.\n",
        "*   `act()` 메쏘드의 경우 $\\epsilon$-greedy 정책에 따라 action을 선택하는 함수입니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aod3D2Lnkw7_"
      },
      "source": [
        "class QLearning:\n",
        "    # 초기값 설정\n",
        "    def __init__(self):\n",
        "        self.action_no = 4 # 가능한 액션 갯수\n",
        "        self.alpha = 0.01 # 반영률\n",
        "        self.gamma = 0.9 # Discount factor\n",
        "        self.epsilon = 0.2 # 입실론 (액션을 취하는 확률)\n",
        "        self.q_values = defaultdict(lambda: [0.0] * self.action_no) # Q테이블\n",
        "\n",
        "    # 업데이트 함수 ( 학습 )\n",
        "    def update(self, state, action, reward, next_state, next_action): \n",
        "        q_value = self.q_values[state][action] # 현재 state에서의 q_value값\n",
        "        next_q_value = max(self.q_values[next_state]) # 다음상태에서 q_value값 (탐욕적 행동을 결정하기에 max 값을 가짐)\n",
        "        td_error = (reward + self.gamma * next_q_value) - q_value # target 정책과, 행동의 정책과의 value 차이\n",
        "        self.q_values[state][action] = q_value + self.alpha * td_error # q_vlaue 업데이트 (반영률을 통해 값 반영)\n",
        "    \n",
        "    # 행동 함수\n",
        "    def act(self, state):\n",
        "        '''\n",
        "        설정한 입실론 값보다, 작을 경우 조건문 참조건이 실행되며, 랜덤으로 가능한 동작 중 하나를 선택합니다.\n",
        "        그 외의 경우, 현재 state가 가지는 모든 Q값 중 가장 큰 값을 가지는 행동을 선택합니다.\n",
        "        '''\n",
        "        return np.random.choice(self.action_no) if np.random.rand() < self.epsilon else np.argmax(self.q_values[state]) # 행동 반환\u001e"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0Jq-C_Vr1HV"
      },
      "source": [
        "아래는 Sarsa 알고리즘을 수행하는 Class의 정의입니다.\n",
        "*   `update()` 메쏘드의 경우 state, action, reward, next_state, next_action이 주어졌을 때 Q-value를 업데이트하는 함수입니다.\n",
        "*   `act()` 메쏘드의 경우 $\\epsilon$-greedy 정책에 따라 action을 선택하는 함수입니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moiYtfA0pYiY"
      },
      "source": [
        "class Sarsa:\n",
        "    def __init__(self):\n",
        "        # 초기값 설정\n",
        "        self.action_no = 4 # 가능한 액션 갯수\n",
        "        self.alpha = 0.01 # 반영률\n",
        "        self.gamma = 0.9 # Discount factor\n",
        "        self.epsilon = 0.2 # 입실론 (액션을 취하는 확률)\n",
        "        self.q_values = defaultdict(lambda: [0.0] * self.action_no)\n",
        "        \n",
        "    # 업데이트 함수 ( 학습 )\n",
        "    def update(self, state, action, reward, next_state, next_action):\n",
        "        q_value = self.q_values[state][action] # 현재 state에서 q_value값\n",
        "        next_q_value = self.q_values[next_state][next_action] # 다음상태에서 q_value값 (Sarsa의 경우 behavior 정책 사용)\n",
        "        td_error = (reward + self.gamma * next_q_value) - q_value # target 정책과, 행동의 정책과의 value 차이\n",
        "        self.q_values[state][action] = q_value + self.alpha * td_error # q_value 업데이트 (반영률 통해 값 반영)\n",
        "\n",
        "    # 행동 함수    \n",
        "    def act(self, state):\n",
        "        '''\n",
        "        설정한 입실론 값보다, 작을 경우 조건문 참조건이 실행되며, 랜덤으로 가능한 동작 중 하나를 선택합니다.\n",
        "        그 외의 경우, 현재 state가 가지는 모든 Q값 중 가장 큰 값을 가지는 행동을 선택합니다.\n",
        "        '''\n",
        "        return np.random.choice(self.action_no) if np.random.random() < self.epsilon else np.argmax(self.q_values[state])"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75i5h8kmr9Mp"
      },
      "source": [
        "OpenAI Gym에서의 Cliff Walking 환경을 로드하고 해당하는 환경을 살펴보기 위해 `render()` 메쏘드를 사용해봅니다.\n",
        "그리고 `env.nS` 및 `env.nA` 변수를 통해 해당 환경의 state 및 action 개수를 확인합니다.\n",
        "\n",
        "Cliff Walking 환경에서 각 state는 grid에서의 위치, action은 'up', 'right', 'down', 'left' 방향을 의미합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orqk6rcLg-uZ",
        "outputId": "5fce9b5c-2e7b-421e-e025-b7a3de8f104c"
      },
      "source": [
        "env = CliffWalkingEnv()\n",
        "env.render()\n",
        "print ('Number of states: ', env.nS)\n",
        "print ('Number of actions :', env.nA)"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n",
            "Number of states:  48\n",
            "Number of actions : 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og4Di2fGll2B"
      },
      "source": [
        "주어진 Q-value에서 greedy policy를 출력하는 QtoPolicy Class를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3H1tlkwnSVu"
      },
      "source": [
        "policy = QtoPolicy()"
      ],
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unvFLwuGsaEQ"
      },
      "source": [
        "Q-learning Class를 정의하고 5000 episode 동안 학습을 수행합니다.\n",
        "\n",
        "Gym 라이브러리의 환경에서는 `step(action)` 메쏘드를 통해 해당하는 time-step에서 action을 수행한 효과를 얻을 수 있습니다. 해당 메쏘드에서는 action을 수행하여 얻어지는 보상 (reward), 다음 상태 (next_state), done (episode 종료여부) 등이 출력으로 주어집니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "patW0RK5olea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e211370-daa0-4d34-bb51-d1a34f6e8692"
      },
      "source": [
        "agent_QL = QLearning()\n",
        "for ep in range(5000):\n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    action = agent_QL.act(state)\n",
        "    \n",
        "    ep_rewards = 0\n",
        "    while not done:\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        next_action = agent_QL.act(next_state)\n",
        "\n",
        "        agent_QL.update(state, action, reward, next_state, next_action)\n",
        "        \n",
        "        ep_rewards += reward\n",
        "        state = next_state\n",
        "        action = next_action\n",
        "    if (ep+1) % 200 == 0:\n",
        "        print(f\"episode: {ep+1}, rewards: {ep_rewards}\")"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 200, rewards: -256\n",
            "episode: 400, rewards: -296\n",
            "episode: 600, rewards: -139\n",
            "episode: 800, rewards: -144\n",
            "episode: 1000, rewards: -144\n",
            "episode: 1200, rewards: -360\n",
            "episode: 1400, rewards: -25\n",
            "episode: 1600, rewards: -33\n",
            "episode: 1800, rewards: -19\n",
            "episode: 2000, rewards: -15\n",
            "episode: 2200, rewards: -17\n",
            "episode: 2400, rewards: -29\n",
            "episode: 2600, rewards: -128\n",
            "episode: 2800, rewards: -35\n",
            "episode: 3000, rewards: -15\n",
            "episode: 3200, rewards: -15\n",
            "episode: 3400, rewards: -33\n",
            "episode: 3600, rewards: -15\n",
            "episode: 3800, rewards: -14\n",
            "episode: 4000, rewards: -123\n",
            "episode: 4200, rewards: -131\n",
            "episode: 4400, rewards: -17\n",
            "episode: 4600, rewards: -323\n",
            "episode: 4800, rewards: -18\n",
            "episode: 5000, rewards: -26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGnPP-qAsoAY"
      },
      "source": [
        "Sarsa에 대해서도 같은 방식으로 학습을 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv2LB-xEqYL4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "effcf443-38d6-4ef0-fdc2-91736063482a"
      },
      "source": [
        "agent_Sa = Sarsa()\n",
        "for ep in range(5000):\n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    action = agent_Sa.act(state)\n",
        "    \n",
        "    ep_rewards = 0\n",
        "    while not done:\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        next_action = agent_Sa.act(next_state)\n",
        "\n",
        "        agent_Sa.update(state, action, reward, next_state, next_action)\n",
        "        \n",
        "        ep_rewards += reward\n",
        "        state = next_state\n",
        "        action = next_action\n",
        "    if (ep+1) % 200 == 0:\n",
        "        print(f\"episode: {ep+1}, rewards: {ep_rewards}\")"
      ],
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 200, rewards: -484\n",
            "episode: 400, rewards: -85\n",
            "episode: 600, rewards: -45\n",
            "episode: 800, rewards: -138\n",
            "episode: 1000, rewards: -22\n",
            "episode: 1200, rewards: -27\n",
            "episode: 1400, rewards: -23\n",
            "episode: 1600, rewards: -26\n",
            "episode: 1800, rewards: -150\n",
            "episode: 2000, rewards: -27\n",
            "episode: 2200, rewards: -20\n",
            "episode: 2400, rewards: -19\n",
            "episode: 2600, rewards: -17\n",
            "episode: 2800, rewards: -122\n",
            "episode: 3000, rewards: -19\n",
            "episode: 3200, rewards: -22\n",
            "episode: 3400, rewards: -140\n",
            "episode: 3600, rewards: -15\n",
            "episode: 3800, rewards: -17\n",
            "episode: 4000, rewards: -22\n",
            "episode: 4200, rewards: -22\n",
            "episode: 4400, rewards: -19\n",
            "episode: 4600, rewards: -27\n",
            "episode: 4800, rewards: -119\n",
            "episode: 5000, rewards: -23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox-__wN3s3LR"
      },
      "source": [
        "학습된 Q-value를 이용하여 학습된 정책을 출력합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83Dgl6fqZkTP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff525c26-ce15-4321-c672-05db10e23ed4"
      },
      "source": [
        "print('Learned policy by Q-learning')\n",
        "policy.printPolicy(agent_QL.q_values)\n",
        "print('Learned policy by Sarsa')\n",
        "policy.printPolicy(agent_Sa.q_values)"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned policy by Q-learning\n",
            "[['←' '↓' '→' '↓' '→' '→' '↑' '↓' '→' '↓' '→' '↓']\n",
            " ['↑' '→' '↑' '↑' '→' '↓' '→' '→' '→' '→' '→' '↓']\n",
            " ['→' '→' '→' '→' '→' '→' '→' '→' '→' '→' '→' '↓']\n",
            " ['↑' '←' '←' '←' '←' '←' '←' '←' '←' '←' '←' '↑']]\n",
            "\n",
            "Learned policy by Sarsa\n",
            "[['→' '→' '→' '→' '→' '→' '→' '→' '→' '→' '→' '↓']\n",
            " ['↑' '↑' '↑' '↑' '→' '→' '→' '→' '→' '→' '→' '↓']\n",
            " ['↑' '↑' '↑' '↑' '↑' '↑' '↑' '↑' '→' '→' '→' '↓']\n",
            " ['↑' '←' '←' '←' '←' '←' '←' '←' '←' '←' '←' '↑']]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NFlcKhvoq4u"
      },
      "source": [
        "env.close()"
      ],
      "execution_count": 229,
      "outputs": []
    }
  ]
}